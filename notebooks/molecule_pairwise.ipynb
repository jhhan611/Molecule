{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:38:40.781159Z",
     "start_time": "2024-05-30T16:38:40.635134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric import edge_index\n",
    "from torch_geometric.transforms import BaseTransform, Compose\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.nn.aggr import SumAggregation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "HERE = Path(\"/Users/USER/PycharmProjects/molecule\")\n",
    "DATA = HERE / \"data\""
   ],
   "id": "8169c2671a8c0e5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\miniconda3\\envs\\molecule\\Lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "invalid type object: only floating-point types are supported as the default type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m torch\u001B[38;5;241m.\u001B[39mset_default_device(device)\n\u001B[1;32m---> 19\u001B[0m torch\u001B[38;5;241m.\u001B[39mset_default_tensor_type(device)\n\u001B[0;32m     20\u001B[0m HERE \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/USER/PycharmProjects/molecule\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m DATA \u001B[38;5;241m=\u001B[39m HERE \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\molecule\\Lib\\site-packages\\torch\\__init__.py:696\u001B[0m, in \u001B[0;36mset_default_tensor_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    695\u001B[0m     t \u001B[38;5;241m=\u001B[39m _import_dotted_name(t)\n\u001B[1;32m--> 696\u001B[0m _C\u001B[38;5;241m.\u001B[39m_set_default_tensor_type(t)\n",
      "\u001B[1;31mTypeError\u001B[0m: invalid type object: only floating-point types are supported as the default type"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:34:17.050175Z",
     "start_time": "2024-05-30T15:33:30.978553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def num_heavy_atoms(qm9_data: Data) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of heavy atoms in a torch geometric\n",
    "    Data object.\n",
    "    \"\"\"\n",
    "    # every atom with atomic number other than 1 is heavy\n",
    "    return (qm9_data.z != 1).sum()\n",
    "\n",
    "qm9_dataset = QM9(\n",
    "    DATA,\n",
    "    # Filter out molecules with more than 8 heavy atoms\n",
    "    pre_filter=lambda data: num_heavy_atoms(data) < 9,\n",
    "    force_reload=True\n",
    ")\n",
    "\n",
    "mx = 0\n",
    "for d in qm9_dataset:\n",
    "    mx = max(mx, d.z.shape[0])\n",
    "\n",
    "data_sample = qm9_dataset[0]\n",
    "data_sample.pos.round(decimals=3)\n",
    "data_sample.y\n",
    "len(qm9_dataset)"
   ],
   "id": "5893ff70b4acc20c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:34:23.606349Z",
     "start_time": "2024-05-30T15:34:23.105584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import periodictable as pt\n",
    "\n",
    "def adjacency_matrix(z: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "    am = torch.zeros(z.shape[0], z.shape[0])\n",
    "    for idx in range(edge_index.shape[1]):\n",
    "        am[idx, idx] = 1\n",
    "        bond = 0\n",
    "        for i in range(3):\n",
    "            bond += edge_attr[idx, i] * (i+1)\n",
    "        am[edge_index[0, idx], edge_index[1, idx]] = bond\n",
    "    return am\n",
    "    \n",
    "def distance_matrix(z: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n",
    "    dm = torch.zeros(z.shape[0], z.shape[0])\n",
    "    for idx1 in range(z.shape[0]):\n",
    "        for idx2 in range(z.shape[0]):\n",
    "            coord1 = pos[idx1]\n",
    "            coord2 = pos[idx2]\n",
    "            dist = (coord1 - coord2).pow(2).sum().sqrt().item()\n",
    "            dm[idx1, idx2] = dist\n",
    "            dm[idx2, idx1] = dist\n",
    "    return dm\n",
    "\n",
    "def display_molecule(pos: torch.Tensor, z: torch.Tensor):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    coord = pos.numpy()\n",
    "    ax.scatter(coord[:, 0], coord[:, 1], coord[:, 2])\n",
    "    for i in range(z.size(0)):\n",
    "        ax.text(coord[i, 0], coord[i, 1], coord[i, 2], \"{}\".format(pt.elements[z[i].item()].symbol))\n",
    "\n",
    "# display_molecule(data_sample.pos, data_sample.z)"
   ],
   "id": "b4503b1f6bdd41b8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T17:03:16.326476Z",
     "start_time": "2024-05-30T17:03:04.353291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "data_list = torch.load('data_pair.pt', map_location=lambda storage, loc: storage.cuda(0))\n",
    "\n",
    "# data_list = []\n",
    "# cnt = 0\n",
    "# \n",
    "# for dt in qm9_dataset:\n",
    "#     pos = dt.pos\n",
    "#     dist = torch.tensor([(pos[i]-pos[j]).pow(2).sum().sqrt().item() for i in range(dt.z.shape[0]) for j in range(i)])\n",
    "#     pairs = torch.tensor([(i, j) for i in range(dt.z.shape[0]) for j in range(i)])\n",
    "#     \n",
    "#     # node level\n",
    "#     pad1 = nn.ZeroPad1d((0, mx*(mx-1)//2-dist.shape[0]))\n",
    "#     pad2 = nn.ZeroPad2d((0, 0, 0, mx*(mx-1)//2-dist.shape[0]))\n",
    "#     data_list.append(Data(x=dt.z.reshape([-1, 1]).float(), edge_index=dt.edge_index, y=pad1(dist), pairs=pad2(pairs)))\n",
    "#     \n",
    "#     cnt += 1\n",
    "#     if cnt % 1000 == 0:\n",
    "#         print(cnt)\n",
    "#     # graph level\n",
    "#     # dist_l = pad(dist_m).flatten()\n",
    "#     # data_list.append(Data(x=dt.z.reshape([-1, 1]).float(), edge_index=dt.edge_index, y=dist_l))\n",
    "\n",
    "mx = 0\n",
    "for d in data_list:\n",
    "    mx = max(mx, d.x.shape[0])\n",
    "\n",
    "l = len(data_list)\n",
    "train_l = data_list[:l*4//5]\n",
    "val_l = data_list[l*4//5+1:l*9//10]\n",
    "test_l = data_list[l*9//10+1:l]\n",
    "\n",
    "batch = 1024\n",
    "train_loader = DataLoader(train_l, batch_size=batch, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "val_loader = DataLoader(val_l, batch_size=batch, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "test_loader = DataLoader(test_l, batch_size=batch, shuffle=True, generator=torch.Generator(device='cuda'))"
   ],
   "id": "b8bf08d95d005e24",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:57:05.957072Z",
     "start_time": "2024-05-30T16:56:56.816820Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(data_list, \"data_pair.pt\")",
   "id": "b680445549df8f2a",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T17:03:22.548123Z",
     "start_time": "2024-05-30T17:03:22.538298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import GraphConv, GINConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GINPair(torch.nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_hidden: int, combine_method='concat'):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(dim_in, dim_hidden), BatchNorm1d(dim_hidden), ReLU(), Linear(dim_hidden, dim_hidden), ReLU())\n",
    "        )\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_hidden, dim_hidden), BatchNorm1d(dim_hidden), ReLU(), Linear(dim_hidden, dim_hidden), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_hidden, dim_hidden), BatchNorm1d(dim_hidden), ReLU(), Linear(dim_hidden, dim_hidden), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.combine_method = combine_method\n",
    "        if combine_method == 'concat':\n",
    "            self.fc = nn.Linear(2 * dim_hidden, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(dim_hidden, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_idx: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.conv1(x, edge_idx)\n",
    "        output = output.relu()\n",
    "        # output = self.conv2(output, edge_idx)\n",
    "        # output = output.relu()\n",
    "        # output = F.dropout(output, p=0.5, training=self.training)\n",
    "        output = self.conv3(output, edge_idx)\n",
    "        output = output.relu()\n",
    "        # output = global_mean_pool(output, None)\n",
    "        # output = self.lin(output)\n",
    "        return output\n",
    "    \n",
    "    def combine(self, node1_emb, node2_emb):\n",
    "        if self.combine_method == 'concat':\n",
    "            return torch.cat([node1_emb, node2_emb], dim=1)\n",
    "        elif self.combine_method == 'sum':\n",
    "            return node1_emb + node2_emb\n",
    "        elif self.combine_method == 'product':\n",
    "            return node1_emb * node2_emb\n",
    "        elif self.combine_method == 'diff':\n",
    "            return torch.abs(node1_emb - node2_emb)\n",
    "        elif self.combine_method == 'average':\n",
    "            return (node1_emb + node2_emb) / 2\n",
    "        elif self.combine_method == 'max':\n",
    "            combined_emb, _ = torch.max(torch.stack([node1_emb, node2_emb]), dim=0)\n",
    "            return combined_emb\n",
    "        \n",
    "    def predict(self, node1_emb, node2_emb):\n",
    "        combined_emb = self.combine(node1_emb, node2_emb)\n",
    "        return self.fc(combined_emb)"
   ],
   "id": "f57fa2aa7fc6c569",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T17:11:47.852219Z",
     "start_time": "2024-05-30T17:11:47.842377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mask(data_y):\n",
    "    return (data_y != 0).float()\n",
    "\n",
    "def train(model, lr):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=lr,\n",
    "                                      weight_decay=5e-4)\n",
    "    \n",
    "    model.train()\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for d in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        node_embed = model(d.x, d.edge_index)\n",
    "        # node1 = torch.tensor([i for i in range(d.x.shape[0]) for _ in range(i)])\n",
    "        # node2 = torch.tensor([j for i in range(d.x.shape[0]) for j in range(i)])\n",
    "        node1 = torch.tensor([pair[0] for pair in d.pairs])\n",
    "        node2 = torch.tensor([pair[1] for pair in d.pairs])\n",
    "        node1_emb = node_embed[node1]\n",
    "        node2_emb = node_embed[node2]\n",
    "        \n",
    "        output = model.predict(node1_emb, node2_emb).squeeze()\n",
    "        l = criterion(output, d.y).mean()\n",
    "        loss += l / len(train_loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cnt += 1\n",
    "        print(f\"{cnt}/{len(train_loader)}\")\n",
    "    return loss, model\n",
    "\n",
    "def validate(model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model.eval()\n",
    "    v_loss = 0\n",
    "    for d in val_loader:\n",
    "        node_embed = model(d.x, d.edge_index)\n",
    "        node1 = torch.tensor([pair[0] for pair in d.pairs])\n",
    "        node2 = torch.tensor([pair[1] for pair in d.pairs])\n",
    "        node1_emb = node_embed[node1]\n",
    "        node2_emb = node_embed[node2]\n",
    "        output = model.predict(node1_emb, node2_emb).squeeze()\n",
    "        l = criterion(output, d.y).mean()\n",
    "        v_loss += l / len(val_loader)\n",
    "    return v_loss\n",
    "        \n",
    "@torch.no_grad()\n",
    "def test(model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    t_loss = 0\n",
    "    for d in test_loader:\n",
    "        output = model(d.x, d.edge_index)\n",
    "        l = (criterion(output, d.y) * mask(d.y)).mean()\n",
    "        t_loss += l / len(test_loader)\n",
    "    return t_loss\n",
    "\n",
    "def train_epoch(epoch, model, lr):\n",
    "    for e in range(epoch):\n",
    "        e_loss, model = train(model, lr)\n",
    "        v_loss = validate(model)\n",
    "        \n",
    "        if e % 2 == 0:\n",
    "            print(f\"Epoch: {e}, train_loss: {e_loss.item()}, val_loss: {v_loss.item()}\")"
   ],
   "id": "c5679ebfb52fb27d",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T17:21:31.269856Z",
     "start_time": "2024-05-30T17:13:38.165563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 20\n",
    "model = GINPair(dim_in=1, dim_hidden=64, combine_method='concat')\n",
    "train_epoch(epochs, model, 0.001)"
   ],
   "id": "7f4e190a0034a9c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/18\n",
      "2/18\n",
      "3/18\n",
      "4/18\n",
      "5/18\n",
      "6/18\n",
      "7/18\n",
      "8/18\n",
      "9/18\n",
      "10/18\n",
      "11/18\n",
      "12/18\n",
      "13/18\n",
      "14/18\n",
      "15/18\n",
      "16/18\n",
      "17/18\n",
      "18/18\n",
      "Epoch: 0, train_loss: 2.758960723876953, val_loss: 3.3880295753479004\n",
      "1/18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m GINPair(dim_in\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, dim_hidden\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, combine_method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconcat\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m train_epoch(epochs, model, \u001B[38;5;241m0.001\u001B[39m)\n",
      "Cell \u001B[1;32mIn[72], line 61\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(epoch, model, lr)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_epoch\u001B[39m(epoch, model, lr):\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch):\n\u001B[1;32m---> 61\u001B[0m         e_loss, model \u001B[38;5;241m=\u001B[39m train(model, lr)\n\u001B[0;32m     62\u001B[0m         v_loss \u001B[38;5;241m=\u001B[39m validate(model)\n\u001B[0;32m     64\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m e \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[72], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, lr)\u001B[0m\n\u001B[0;32m     15\u001B[0m node_embed \u001B[38;5;241m=\u001B[39m model(d\u001B[38;5;241m.\u001B[39mx, d\u001B[38;5;241m.\u001B[39medge_index)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# node1 = torch.tensor([i for i in range(d.x.shape[0]) for _ in range(i)])\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# node2 = torch.tensor([j for i in range(d.x.shape[0]) for j in range(i)])\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m node1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([pair[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m pair \u001B[38;5;129;01min\u001B[39;00m d\u001B[38;5;241m.\u001B[39mpairs])\n\u001B[0;32m     19\u001B[0m node2 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([pair[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m pair \u001B[38;5;129;01min\u001B[39;00m d\u001B[38;5;241m.\u001B[39mpairs])\n\u001B[0;32m     20\u001B[0m node1_emb \u001B[38;5;241m=\u001B[39m node_embed[node1]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\molecule\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001B[0m, in \u001B[0;36mDeviceContext.__torch_function__\u001B[1;34m(self, func, types, args, kwargs)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m _device_constructors() \u001B[38;5;129;01mand\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     76\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "   # TODO: exclude 0 paddings from loss calculation\n",
    "\"\"\"\n",
    "Epoch: 0, Train loss: 0.03952614217996597, Val loss: 0.04244199022650719\n",
    "Epoch: 2, Train loss: 0.03944431617856026, Val loss: 0.04346185922622681\n",
    "Epoch: 4, Train loss: 0.0394737534224987, Val loss: 0.04332466423511505\n",
    "Epoch: 6, Train loss: 0.03944838419556618, Val loss: 0.04295289143919945\n",
    "Epoch: 8, Train loss: 0.03944433107972145, Val loss: 0.042868148535490036\n",
    "Time: 10m 21s 67ms\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Epoch: 0, train_loss: 2.819444417953491, val_loss: 1.3333452939987183\n",
    "Epoch: 2, train_loss: 1.0538444519042969, val_loss: 0.8541600108146667\n",
    "Epoch: 4, train_loss: 1.0235944986343384, val_loss: 0.8158113956451416\n",
    "Epoch: 6, train_loss: 1.0137405395507812, val_loss: 0.8021357655525208\n",
    "Epoch: 8, train_loss: 1.0096156597137451, val_loss: 0.7891163229942322\n",
    "Epoch: 10, train_loss: 1.0031167268753052, val_loss: 0.7599336504936218\n",
    "Epoch: 12, train_loss: 1.0010273456573486, val_loss: 0.8162444233894348\n",
    "Epoch: 14, train_loss: 0.9971020221710205, val_loss: 0.7471166849136353\n",
    "Epoch: 16, train_loss: 0.9963588714599609, val_loss: 0.7429596185684204\n",
    "Epoch: 18, train_loss: 0.9939806461334229, val_loss: 0.7843343615531921\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Epoch: 0, train_loss: 2.607553482055664, val_loss: 1.5287598371505737\n",
    "Epoch: 2, train_loss: 0.9871993064880371, val_loss: 0.8298434019088745\n",
    "Epoch: 4, train_loss: 0.9566546678543091, val_loss: 0.8694849610328674\n",
    "Epoch: 6, train_loss: 0.9433390498161316, val_loss: 0.7554209232330322\n",
    "Epoch: 8, train_loss: 0.9362509250640869, val_loss: 0.8846598267555237\n",
    "Epoch: 10, train_loss: 0.9299172163009644, val_loss: 0.8427674770355225\n",
    "Epoch: 12, train_loss: 0.926454484462738, val_loss: 0.7863412499427795\n",
    "Epoch: 14, train_loss: 0.9211176633834839, val_loss: 1.0939804315567017\n",
    "Epoch: 16, train_loss: 0.917726457118988, val_loss: 0.8830863237380981\n",
    "Epoch: 18, train_loss: 0.9147141575813293, val_loss: 0.7525851130485535\n",
    "Epoch: 20, train_loss: 0.9135162234306335, val_loss: 0.7463304400444031\n",
    "Epoch: 22, train_loss: 0.9099758267402649, val_loss: 0.760979950428009\n",
    "Epoch: 24, train_loss: 0.907063901424408, val_loss: 0.7801485061645508\n",
    "Epoch: 26, train_loss: 0.905415415763855, val_loss: 0.7539505362510681\n",
    "Epoch: 28, train_loss: 0.905021607875824, val_loss: 0.8408609628677368\n",
    "Epoch: 30, train_loss: 0.9012361764907837, val_loss: 0.9827206134796143\n",
    "Epoch: 32, train_loss: 0.8998003005981445, val_loss: 0.9063634872436523\n",
    "Epoch: 34, train_loss: 0.8969922661781311, val_loss: 0.783204197883606\n",
    "Epoch: 36, train_loss: 0.8964552283287048, val_loss: 0.7484853267669678\n",
    "Epoch: 38, train_loss: 0.8951149582862854, val_loss: 0.7687615156173706\n",
    "Epoch: 40, train_loss: 0.8946443200111389, val_loss: 0.8196905851364136\n",
    "Epoch: 42, train_loss: 0.892460286617279, val_loss: 0.7178381681442261\n",
    "Epoch: 44, train_loss: 0.8909822106361389, val_loss: 1.2139568328857422\n",
    "Epoch: 46, train_loss: 0.8888286352157593, val_loss: 0.7084630727767944\n",
    "Epoch: 48, train_loss: 0.8880488276481628, val_loss: 0.7248218059539795\n",
    "\"\"\"\n",
    "# Epoch: 46, train_loss: 0.8888286352157593, val_loss: 0.7084630727767944"
   ],
   "id": "5bf4b99fe88c47c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
